# template/example drop-in file for onto-search setup with self-deployable text2vec-transformers using huggingface models that are compatible with any model that is compatible with the Transformer library's AutoModel and AutoTokenizer classes.
# you can use on of the examples service as template; change the service-name and adjust MODEL_NAME, exposed ports (if needed) and optionally environment var for startup of the model
# showcased here are 2 transformers models, one of them is a sentence-transformers model, the other one is a regular on

version: '2.4'

services:
  weaviate:
    environment:
      ENABLE_MODULES: 'text2vec-transformers'
      TRANSFORMERS_INFERENCE_API: 'http://custom-t2v-hf-transformer:8080'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      
  custom-t2v-hf-transformer:
    environment: # see https://github.com/weaviate/t2v-transformers-models/tree/d7d8312d5b58d3f7fd5220485961d8ca746c2d57 for environment vars in download.py and Dockerfile
      MODEL_NAME: 'distilbert/distilroberta-base'  # hugginface transformers model name
      ENABLE_CUDA: '0'
    extends: # use "template" service that only loads the model on first run and then uses the cached model 
      file: docker-compose.custom-hf-base-config.yml
      service: custom-t2v-hf-transformer-base-config

  custom-t2v-hf-sentence-transformer:
    environment:
      MODEL_NAME: 'sentence-transformers/all-MiniLM-L6-v2'
      USE_SENTENCE_TRANSFORMERS_VECTORIZER: '1' # use sentence-transformers vectorizer for native sentence-transformers models
      ENABLE_CUDA: '0'
    extends: # use "template" service that only loads the model on first run and then uses the cached model 
      file: docker-compose.custom-hf-base-config.yml
      service: custom-t2v-hf-transformer-base-config
     # ports:
     #   - "8089:8080"

