# template/example drop-in file for onto-search setup with self-deployable text2vec-transformers using huggingface models that are compatible with any model that is compatible with the Transformer library's AutoModel and AutoTokenizer classes.
# you can use on of the examples service as template; change the service-name and adjust MODEL_NAME, exposed ports (if needed) and optionally environment var for startup of the model
# showcased here are 2 transformers models, one of them is a sentence-transformers model, the other one is a regular on

version: '3.8'

services:
      
  custom-t2v-hf-transformer:
    environment: # see https://github.com/weaviate/t2v-transformers-models/tree/d7d8312d5b58d3f7fd5220485961d8ca746c2d57 for environment vars in download.py and Dockerfile
      MODEL_NAME: 'distilbert/distilroberta-base'  # hugginface transformers model name
    extends: # use "template" service that only loads the model on first run and then uses the cached model 
      file: docker-compose.custom-hf-base-config${T2VTRANSFORMER_BASE_CONF_EXTENSION}.yml # use the base-config file that is either default (cpu) or
      service: custom-t2v-hf-transformer-base-config                                      # or an extension e.g.  .gpu (if gpu is supposed to be used)
    ports:
       - "8087:8080"

  custom-t2v-hf-sentence-transformer:
    environment:
      MODEL_NAME: 'sentence-transformers/all-MiniLM-L6-v2'
      USE_SENTENCE_TRANSFORMERS_VECTORIZER: '1' # use sentence-transformers vectorizer for native sentence-transformers models
    extends: # use "template" service that only loads the model on first run and then uses the cached model 
      file: docker-compose.custom-hf-base-config${T2VTRANSFORMER_BASE_CONF_EXTENSION}.yml # use the base-config file that is either default (cpu) or
      service: custom-t2v-hf-transformer-base-config                                      # or set an extension e.g. .gpu (if gpu is supposed to be used)  
    ports:
       - "8089:8080"

